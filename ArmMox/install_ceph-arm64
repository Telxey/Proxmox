#!/bin/bash

# ANSI color codes
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
BLUE='\033[0;34m'
MAGENTA='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Ceph version
ceph_version="19.2.0-pve2"

# Function to prompt for user input
prompt() {
    echo -e "${CYAN}$1: ${NC}" >&2
    read -p "" input
    echo "$input"
}

# Function to generate a random FSID
generate_fsid() {
    echo "$(uuidgen)"
}

# Function to validate IP address
validate_ip() {
    if [[ $1 =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$ ]] || [[ $1 =~ ^[a-fA-F0-9:]+$ ]]; then
        return 0
    else
        return 1
    fi
}

# Function to validate network CIDR
validate_cidr() {
    if [[ $1 =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+/[0-9]+$ ]] || [[ $1 =~ ^[a-fA-F0-9:]+/[0-9]+$ ]]; then
        return 0
    else
        return 1
    fi
}

# Function to add Ceph repository
add_ceph_repo() {
    local host=$1
    echo -e "${YELLOW}Adding Ceph repository on $host...${NC}"
    ssh "$host" "cat <<EOF | tee /etc/apt/sources.list.d/ceph.list > /dev/null
deb [arch=arm64] https://mirrors.apqa.cn/proxmox/debian/pve bookworm ceph-squid
EOF"
}

# Function to install Ceph packages
install_ceph_packages() {
    local host=$1
    echo -e "${YELLOW}Installing Ceph packages on $host...${NC}"
    ssh "$host" "apt update && apt install -y --reinstall \
        ceph=$ceph_version \
        ceph-common=$ceph_version \
        ceph-base=$ceph_version \
        ceph-mgr=$ceph_version \
        ceph-mds=$ceph_version \
        ceph-osd=$ceph_version \
        ceph-volume=$ceph_version \
        ceph-mon=$ceph_version \
        ceph-mgr-dashboard=$ceph_version \
        radosgw \
        python3-pip \
        python3-wheel \
        python3-cryptography \
        python3-jwt \
        python3-openssl \
        python3-cherrypy3 \
        python3-bcrypt \
        python3-werkzeug \
        python3-requests \
        python3-routes"
}

# Function to configure Ceph
configure_ceph() {
    local host=$1
    echo -e "${YELLOW}Configuring Ceph on $host...${NC}"
    ssh "$host" "echo 'PYTHON_PATH=\"/usr/lib/python3/dist-packages\"' >> /etc/default/ceph"
    ssh "$host" "chown -R ceph:ceph /usr/share/ceph/mgr/"
}

# Function to create Ceph configuration
create_ceph_conf() {
    local fsid=$1
    local cluster_network=$2
    local public_network=$3
    local mon_hosts=$4

    echo -e "${YELLOW}Creating Ceph configuration...${NC}"
    cat <<EOF > /etc/pve/ceph.conf
[global]
    auth_client_required = cephx
    auth_cluster_required = cephx
    auth_service_required = cephx
    cephx_cluster_require_signatures = true
    cephx_require_signatures = true
    cephx_service_require_signatures = true
    cluster_network = $cluster_network
    fsid = $fsid
    mon_allow_pool_delete = true
    mon_host = $mon_hosts
    ms_bind_ipv4 = true
    ms_bind_ipv6 = false
    osd_memory_target = 4294967296
    osd_pool_default_min_size = 2
    osd_pool_default_size = 3
    public_network = $public_network
    rbd_cache = true
    rbd_cache_max_dirty = 67108864
    rbd_cache_size = 134217728
    rbd_cache_target_dirty = 33554432

[client]
    keyring = /etc/pve/priv/\$cluster.\$name.keyring
    rbd_cache = true
    rbd_cache_max_dirty = 67108864
    rbd_cache_size = 134217728
    rbd_cache_target_dirty = 33554432
    rbd_cache_writethrough_until_flush = true

[client.crash]
    keyring = /etc/pve/ceph/\$cluster.\$name.keyring

[client.proxmox]
    key = /etc/ceph/ceph.client.proxmox.keyring

[mds]
    keyring = /var/lib/ceph/mds/ceph-\$id/keyring

[mon]
    mon_data_avail_crit = 5
    mon_data_avail_warn = 30
    mon_memory_target = 2147483648
    mon_osd_max_split_count = 32
    mon_osd_min_in_ratio = 0.75

[osd]
    bluestore_cache_size = 3221225472
    bluestore_cache_size_hdd = 1073741824
    bluestore_cache_size_ssd = 3221225472
    osd_max_backfills = 1
    osd_memory_target_autotune = true
    osd_op_num_shards = 8
    osd_op_num_threads_per_shard = 2
    osd_recovery_max_active = 3
    osd_recovery_op_priority = 3

[mgr]
    mgr_initial_modules = dashboard,status,prometheus
EOF
}

# Function to create OSDs
create_osds() {
    local nodes=("$@")

    for node in "${nodes[@]}"; do
        echo -e "${YELLOW}Setting up OSD on $node...${NC}"

        # List available disks
        echo -e "${CYAN}Available disks on $node:${NC}"
        ssh "$node" "lsblk -d -o NAME,SIZE,MODEL"

        # Prompt for disk selection
        disk=$(prompt "Enter the disk to use for OSD (e.g., /dev/sdb):")

        # Prompt for disk type (SSD, HDD, NVMe)
        disk_type=$(prompt "Enter disk type (ssd, hdd, nvme):")

        # Create OSD with the selected disk and tag
        echo -e "${YELLOW}Creating OSD on $node with disk $disk (type: $disk_type)...${NC}"
        ssh "$node" "ceph-volume lvm create --data $disk --osd-id-allocator=random"

        # Tag the OSD with the disk type
        osd_id=$(ssh "$node" "ceph-volume lvm list --format=json" | jq -r '.[] | select(.devices[] == "'$disk'") | .tags["ceph.osd_id"]')
        if [[ -n $osd_id ]]; then
            echo -e "${GREEN}Tagging OSD $osd_id as $disk_type...${NC}"
            ceph osd crush set-device-class "$disk_type" "$osd_id"
        else
            echo -e "${RED}Failed to find OSD ID for disk $disk on $node.${NC}"
        fi
    done

    # Verify OSDs are up
    echo -e "${YELLOW}Waiting for OSDs to come up...${NC}"
    sleep 10
    ceph osd status
}

# Function to configure RGW
configure_rgw() {
    local nodes=("$@")

    # Create RGW pool
    echo -e "${YELLOW}Creating RGW pool...${NC}"
    ceph osd pool create rgw.data 32 32
    ceph osd pool application enable rgw.data rgw

    # Generate RGW keys
    RGW_access_key=$(openssl rand -hex 16)
    RGW_secret_key=$(openssl rand -hex 32)

    # Create RGW user
    echo -e "${YELLOW}Creating RGW user...${NC}"
    radosgw-admin user create --uid=s3admin \
        --display-name="S3 Admin" \
        --access-key="$RGW_access_key" \
        --secret-key="$RGW_secret_key"

    # Configure RGW in dashboard
    echo -e "${YELLOW}Configuring RGW in dashboard...${NC}"
    ceph config-key set mgr/dashboard/RGW_API_ACCESS_KEY "$RGW_access_key"
    ceph config-key set mgr/dashboard/RGW_API_SECRET_KEY "$RGW_secret_key"
    ceph config-key set mgr/dashboard/RGW_API_SCHEME "https"
    ceph config-key set mgr/dashboard/RGW_API_HOSTS "$(IFS=, ; echo "${nodes[*]}")"
    ceph config-key set mgr/dashboard/RGW_API_PORT "7443"
    ceph config-key set mgr/dashboard/RGW_API_SSL_VERIFY "false"

    # Add RGW configuration to ceph.conf
    for node in "${nodes[@]}"; do
        cat <<EOF >> /etc/pve/ceph.conf
[client.rgw.$node]
    admin_socket = /var/run/ceph/ceph-client.rgw.\$(hostname).asok
    rgw_frontends = "beast ssl_endpoint=192.168.0.11:7443 ssl_certificate=/etc/ceph/certs/rgw.$node.crt ssl_private_key=/etc/ceph/certs/rgw.$node.key"
    host = "$node"
    keyring = /etc/pve/priv/ceph.client.rgw.$node.keyring
    log_file = /var/log/ceph/client.radosgw.\$host.log
    rgw_enable_apis = "s3"
    rgw_thread_pool_size = 256
    rgw_num_rados_handles = 4
    rgw_max_chunk_size = 262144
    rgw_cache_enabled = true
    rgw_cache_lru_size = 10000
EOF
    done

    # Distribute configuration to all nodes
    for node in "${nodes[@]}"; do
        scp /etc/pve/ceph.conf "$node:/etc/pve/ceph.conf"
        ssh "$node" "rm -f /etc/ceph/ceph.conf && ln -s /etc/pve/ceph.conf /etc/ceph/ceph.conf"
    done

    # Start RGW services
    for node in "${nodes[@]}"; do
        ssh "$node" "systemctl start ceph-radosgw@rgw.$node"
        ssh "$node" "systemctl enable ceph-radosgw@rgw.$node"
    done
}

# Main script execution
echo -e "${GREEN}Ceph Cluster Setup Script${NC}"

# Prompt for node names, cluster IPs, and public IPs
declare -A node_cluster_ips
declare -A node_public_ips
node_count=1
while true; do
    node=$(prompt "Enter node $node_count name (or 'done' to finish)")
    if [[ $node == "done" ]]; then
        break
    fi
    while true; do
        cluster_ip=$(prompt "Enter cluster IP address for $node")
        if validate_ip "$cluster_ip"; then
            node_cluster_ips["$node"]="$cluster_ip"
            break
        else
            echo -e "${RED}Invalid cluster IP address. Please try again.${NC}"
        fi
    done
    while true; do
        public_ip=$(prompt "Enter public IP address for $node")
        if validate_ip "$public_ip"; then
            node_public_ips["$node"]="$public_ip"
            break
        else
            echo -e "${RED}Invalid public IP address. Please try again.${NC}"
        fi
    done
    node_count=$((node_count + 1))
done

# Prompt for cluster and public networks
while true; do
    cluster_network=$(prompt "Enter cluster network (CIDR format, e.g., 10.10.10.0/24)")
    if validate_cidr "$cluster_network"; then
        break
    else
        echo -e "${RED}Invalid CIDR format. Please try again.${NC}"
    fi
done

while true; do
    public_network=$(prompt "Enter public network (CIDR format, e.g., 192.168.0.0/24 or fefa:bebe:cafe::/64)")
    if validate_cidr "$public_network"; then
        break
    else
        echo -e "${RED}Invalid CIDR format. Please try again.${NC}"
    fi
done

# Generate FSID
fsid=$(generate_fsid)
echo -e "${GREEN}Generated FSID: $fsid${NC}"

# Add Ceph repository and install packages
for node in "${!node_cluster_ips[@]}"; do
    echo -e "${YELLOW}Setting up $node...${NC}"
    add_ceph_repo "$node"
    install_ceph_packages "$node"
    configure_ceph "$node"
done

# Create Ceph configuration
mon_hosts=$(IFS=" " ; echo "${node_cluster_ips[*]}")
create_ceph_conf "$fsid" "$cluster_network" "$public_network" "$mon_hosts"

# Create OSDs
create_osds "${!node_cluster_ips[@]}"

# Configure RGW
configure_rgw "${!node_cluster_ips[@]}"

echo -e "${GREEN}Ceph cluster setup completed!${NC}"
